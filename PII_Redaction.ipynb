{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Step 1: Upload a Document</h2>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "FileUpload(value=(), accept='.docx,.pdf', description='Upload')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d445fda6e0264f228164d31335286cd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Step 2: Add Manual Entries (Optional)</h2>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Text(value='', description='Name:'), Button(description='Add Name', style=ButtonStyle()), Text(â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4372ff52c8af4f2686f5f1d3e454fff4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Step 3: Process Document</h2>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Button(description='Process and Anonymize', style=ButtonStyle())",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a59fe4413ca4208be09dc5205b5d695"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "221e850e6537450ea110fd9e6e777b18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "import csv\n",
    "import re\n",
    "from io import StringIO, BytesIO\n",
    "import base64\n",
    "from ipywidgets import FileUpload, Button, Output, Text, VBox\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Storage for manual entries\n",
    "names_to_hide = set()\n",
    "companies_to_hide = set()\n",
    "\n",
    "def split_into_chunks(text, max_tokens=150):\n",
    "    \"\"\"Split text into smaller chunks\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for line in lines:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', line)\n",
    "        for sentence in sentences:\n",
    "            if not sentence.strip():  # Skip empty sentences\n",
    "                continue\n",
    "            sentence_tokens = sentence.split()\n",
    "            token_count = len(sentence_tokens)\n",
    "\n",
    "            if current_tokens + token_count > max_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \"\\n\"\n",
    "                current_tokens = token_count\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_tokens += token_count\n",
    "\n",
    "        if current_chunk:  # Add line break between lines\n",
    "            current_chunk += \"\\n\"\n",
    "            current_tokens += 1\n",
    "\n",
    "    if current_chunk:  # Add the last chunk\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_text_from_file(file_content, file_type):\n",
    "    if file_type == 'docx':\n",
    "        doc = Document(BytesIO(file_content))\n",
    "        return \"\\n\".join(p.text for p in doc.paragraphs if p.text)\n",
    "    elif file_type == 'pdf':\n",
    "        pdf = PdfReader(BytesIO(file_content))\n",
    "        return \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "def add_to_hide(entry, entry_type):\n",
    "    if entry_type == 'name':\n",
    "        names_to_hide.add(entry.lower())\n",
    "    elif entry_type == 'company':\n",
    "        companies_to_hide.add(entry.lower())\n",
    "\n",
    "def anonymize_text(text):\n",
    "    try:\n",
    "        # Get model predictions\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Process predictions\n",
    "        tokens = tokenizer.encode_plus(text, return_offsets_mapping=True)\n",
    "        offset_mapping = tokens['offset_mapping']\n",
    "\n",
    "        result = list(text)\n",
    "        found_items = []\n",
    "\n",
    "        # Handle model predictions\n",
    "        for idx, (start, end) in enumerate(offset_mapping):\n",
    "            if idx >= len(predictions[0]):\n",
    "                break\n",
    "            if start == end:\n",
    "                continue\n",
    "\n",
    "            label_id = predictions[0][idx].item()\n",
    "            if label_id != model.config.label2id['O']:\n",
    "                label = model.config.id2label[label_id]\n",
    "                found_items.append((label, text[start:end]))\n",
    "                for i in range(start, end):\n",
    "                    result[i] = ''\n",
    "                result[start] = f'[{label}]'\n",
    "\n",
    "        # Handle manual entries\n",
    "        for name in names_to_hide:\n",
    "            pos = text.lower().find(name)\n",
    "            while pos != -1:\n",
    "                end = pos + len(name)\n",
    "                found_items.append(('I-GIVENNAME', name))\n",
    "                for i in range(pos, end):\n",
    "                    result[i] = ''\n",
    "                result[pos] = '[I-GIVENNAME]'\n",
    "                pos = text.lower().find(name, end)\n",
    "\n",
    "        for company in companies_to_hide:\n",
    "            pos = text.lower().find(company)\n",
    "            while pos != -1:\n",
    "                end = pos + len(company)\n",
    "                found_items.append(('I-ORG', company))\n",
    "                for i in range(pos, end):\n",
    "                    result[i] = ''\n",
    "                result[pos] = '[I-ORG]'\n",
    "                pos = text.lower().find(company, end)\n",
    "\n",
    "        return ''.join(result), found_items\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return text, []\n",
    "\n",
    "def create_csv(chunks_data):\n",
    "    output = StringIO()\n",
    "    writer = csv.writer(output, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(['Chunk Number', 'Original Text', 'Anonymized Text', 'Found Items'])\n",
    "\n",
    "    for i, data in enumerate(chunks_data, 1):\n",
    "        writer.writerow([\n",
    "            f\"Chunk {i}\",\n",
    "            data['original'],\n",
    "            data['anonymized'],\n",
    "            data['found_items']\n",
    "        ])\n",
    "\n",
    "    csv_string = output.getvalue()\n",
    "    b64 = base64.b64encode(csv_string.encode()).decode()\n",
    "    return f'<a href=\"data:text/csv;base64,{b64}\" download=\"anonymized_text.csv\">Download CSV</a>'\n",
    "\n",
    "# Setup Jupyter widgets\n",
    "upload = FileUpload(accept='.docx,.pdf', multiple=False)\n",
    "process_button = Button(description=\"Process and Anonymize\")\n",
    "output = Output()\n",
    "\n",
    "name_input = Text(description='Name:')\n",
    "company_input = Text(description='Company:')\n",
    "add_name_button = Button(description=\"Add Name\")\n",
    "add_company_button = Button(description=\"Add Company\")\n",
    "\n",
    "def on_add_name_clicked(b):\n",
    "    add_to_hide(name_input.value, 'name')\n",
    "    name_input.value = ''\n",
    "\n",
    "def on_add_company_clicked(b):\n",
    "    add_to_hide(company_input.value, 'company')\n",
    "    company_input.value = ''\n",
    "\n",
    "def on_process_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if not upload.value:\n",
    "            print(\"Please upload a file first.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            file_info = upload.value[0]\n",
    "            content = file_info['content']\n",
    "            filename = file_info['name']\n",
    "\n",
    "            # Get file type and process\n",
    "            if filename.endswith('.docx'):\n",
    "                text = get_text_from_file(content, 'docx')\n",
    "            elif filename.endswith('.pdf'):\n",
    "                text = get_text_from_file(content, 'pdf')\n",
    "            else:\n",
    "                raise ValueError(\"Please upload a .docx or .pdf file\")\n",
    "\n",
    "            # Split text into chunks\n",
    "            chunks = split_into_chunks(text)\n",
    "            print(f\"Processing {len(chunks)} chunks...\")\n",
    "\n",
    "            # Process each chunk\n",
    "            chunks_data = []\n",
    "            all_found = []\n",
    "\n",
    "            for i, chunk in enumerate(chunks, 1):\n",
    "                anonymized, found = anonymize_text(chunk)\n",
    "                all_found.extend(found)\n",
    "\n",
    "                chunks_data.append({\n",
    "                    'original': chunk,\n",
    "                    'anonymized': anonymized,\n",
    "                    'found_items': ', '.join(f\"{label}: {item}\" for label, item in found)\n",
    "                })\n",
    "\n",
    "                # Display results for this chunk\n",
    "                display(HTML(f\"<h3>Chunk {i}:</h3>\"))\n",
    "                display(HTML(f\"<p><strong>Original:</strong><br>{chunk.replace(chr(10), '<br>')}</p>\"))\n",
    "                display(HTML(f\"<p><strong>Anonymized:</strong><br>{anonymized.replace(chr(10), '<br>')}</p>\"))\n",
    "                if found:\n",
    "                    display(HTML(f\"<p><strong>Found Items:</strong><br>{chunks_data[-1]['found_items']}</p>\"))\n",
    "                display(HTML(\"<hr>\"))\n",
    "\n",
    "            # Show summary\n",
    "            if all_found:\n",
    "                display(HTML(\"<h3>All Found Items:</h3>\"))\n",
    "                display(HTML(f\"<p>{', '.join(f'{label}: {item}' for label, item in all_found)}</p>\"))\n",
    "\n",
    "            # Create download link\n",
    "            display(HTML(create_csv(chunks_data)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Setup widget interactions\n",
    "add_name_button.on_click(on_add_name_clicked)\n",
    "add_company_button.on_click(on_add_company_clicked)\n",
    "process_button.on_click(on_process_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(HTML(\"<h2>Step 1: Upload a Document</h2>\"))\n",
    "display(upload)\n",
    "display(HTML(\"<h2>Step 2: Add Manual Entries (Optional)</h2>\"))\n",
    "display(VBox([name_input, add_name_button, company_input, add_company_button]))\n",
    "display(HTML(\"<h2>Step 3: Process Document</h2>\"))\n",
    "display(process_button)\n",
    "display(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
